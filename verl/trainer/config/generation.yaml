trainer:
  nnodes: 1                  # Number of nodes (machines) in distributed training; 1 means single machine.
  n_gpus_per_node: 1         # Number of GPUs per node; 8 means each node uses 8 GPUs.
  wandb: False              # Whether to use Weights and Biases for logging; False means no logging.
  project_name: generation  # Project name for Weights and Biases; default is "generation".

data:
  path: ~/data/rlhf/math/test.parquet  # Path to the dataset, a Parquet file for RLHF math data.
  prompt_key: prompt         # Field name in the dataset for prompts.
  n_samples: 5               # Number of samples to generate per prompt; 5 means 5 outputs per input.
  output_path: /opt/tiger/math_Qwen2-7B-Instruct.parquet  # Path to save generated results.
  batch_size: 128            # Batch size for processing data; 128 samples per batch.

model:
  path: ~/models/Qwen2-7B-Instruct  # Path to the model; uses Qwen 2.5 7B Instruct model.
  external_lib: null         # External library to import; null means none specified.
  quantization: bitsandbytes         # Quantization method; bitsandbytes for 4-bit quantization.

actor:
  ulysses_sequence_parallel_size: 1  # Sequence parallel size for Ulysses; 1 means no sequence parallelism.
  fsdp_config:               # Configuration for Fully Sharded Data Parallel (FSDP).
  dtype: bfloat16          # Data type for FSDP; bfloat16 for mixed precision training.
  use_dynamic_bsz: True    # Whether to use dynamic batch size; True enables dynamic batch size.

rollout:
  name: vllm                 # Generation engine type; vLLM for efficient inference.
  temperature: 0.3           # Sampling temperature; 1.0 controls text randomness (higher = more random).
  top_k: 50                  # Top-K sampling; 50 means sample from top 50 probable tokens.
  top_p: 0.7                 # Top-P (nucleus) sampling; 0.7 means sample from tokens with 70% cumulative probability.
  prompt_length: 2048        # Maximum length of input prompts; 1536 tokens.
  response_length: 1024       # Maximum length of generated responses; 512 tokens.
  n: 2                       # Number of sequences to generate per prompt; 2 outputs.
  # For vLLM rollout
  dtype: bfloat16            # Data type for inference; bfloat16, should match FSDP dtype.
  gpu_memory_utilization: 0.5  # GPU memory usage ratio; 0.5 means 50% of GPU memory.
  ignore_eos: False          # Whether to ignore end-of-sequence token; False respects EOS during generation.
  micro_batch_size: 256      # Micro-batch size for processing; 256 samples per micro-batch.
  enforce_eager: True        # Force PyTorch eager mode; True disables compilation optimizations.
  free_cache_engine: True    # Free cache engine after use; True helps manage memory.
  load_format: dummy_dtensor # Model loading format; dummy_dtensor is a vLLM placeholder.
  tensor_model_parallel_size: 1  # Tensor parallel size; 1 means no tensor parallelism (suitable for single GPU).
  max_num_batched_tokens: 8192  # Maximum tokens in a batch; 8192 controls batch size limit.
  max_num_seqs: 1024         # Maximum sequences in a batch; 1024 limits parallel sequences.
  log_prob_micro_batch_size: 8  # Micro-batch size for log probability computation; 8 for RL training steps.
  # For HF rollout (not used here, but included for reference)
  do_sample: True            # Enable sampling; True uses random sampling instead of greedy decoding.