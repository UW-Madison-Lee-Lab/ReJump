/data/yzeng58/anaconda3/envs/liftr/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: yuchen-zeng (lee-lab-uw-madison) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /data/yzeng58/liftr/wandb/run-20250323_210616-trn9jewo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silvery-waterfall-27
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lee-lab-uw-madison/liftr-rl
wandb: üöÄ View run at https://wandb.ai/lee-lab-uw-madison/liftr-rl/runs/trn9jewo
2025-03-23 21:06:18,334	INFO worker.py:1841 -- Started a local Ray instance.
[36m(pid=3284823)[0m /data/yzeng58/anaconda3/envs/liftr/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
[36m(pid=3284823)[0m   warnings.warn(
[36m(main_task pid=3284823)[0m {'actor_rollout_ref': {'actor': {'clip_ratio': 0.2,
[36m(main_task pid=3284823)[0m                                  'entropy_coeff': 0.001,
[36m(main_task pid=3284823)[0m                                  'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=3284823)[0m                                                  'grad_offload': False,
[36m(main_task pid=3284823)[0m                                                  'optimizer_offload': False,
[36m(main_task pid=3284823)[0m                                                  'param_offload': False,
[36m(main_task pid=3284823)[0m                                                  'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=3284823)[0m                                  'grad_clip': 1.0,
[36m(main_task pid=3284823)[0m                                  'kl_loss_coef': 0.001,
[36m(main_task pid=3284823)[0m                                  'kl_loss_type': 'low_var_kl',
[36m(main_task pid=3284823)[0m                                  'optim': {'lr': 1e-06,
[36m(main_task pid=3284823)[0m                                            'lr_warmup_steps_ratio': 0.0,
[36m(main_task pid=3284823)[0m                                            'min_lr_ratio': None,
[36m(main_task pid=3284823)[0m                                            'total_training_steps': -1,
[36m(main_task pid=3284823)[0m                                            'warmup_style': 'constant'},
[36m(main_task pid=3284823)[0m                                  'ppo_epochs': 1,
[36m(main_task pid=3284823)[0m                                  'ppo_max_token_len_per_gpu': 16384,
[36m(main_task pid=3284823)[0m                                  'ppo_micro_batch_size': 4,
[36m(main_task pid=3284823)[0m                                  'ppo_mini_batch_size': 64,
[36m(main_task pid=3284823)[0m                                  'shuffle': False,
[36m(main_task pid=3284823)[0m                                  'strategy': 'fsdp',
[36m(main_task pid=3284823)[0m                                  'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=3284823)[0m                                  'use_dynamic_bsz': False,
[36m(main_task pid=3284823)[0m                                  'use_kl_loss': True},
[36m(main_task pid=3284823)[0m                        'hybrid_engine': True,
[36m(main_task pid=3284823)[0m                        'model': {'enable_gradient_checkpointing': True,
[36m(main_task pid=3284823)[0m                                  'external_lib': None,
[36m(main_task pid=3284823)[0m                                  'override_config': {},
[36m(main_task pid=3284823)[0m                                  'path': 'Qwen/Qwen2.5-3B-Instruct',
[36m(main_task pid=3284823)[0m                                  'use_remove_padding': True},
[36m(main_task pid=3284823)[0m                        'ref': {'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=3284823)[0m                                                'param_offload': True,
[36m(main_task pid=3284823)[0m                                                'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=3284823)[0m                                'log_prob_max_token_len_per_gpu': 16384,
[36m(main_task pid=3284823)[0m                                'log_prob_micro_batch_size': 2,
[36m(main_task pid=3284823)[0m                                'log_prob_use_dynamic_bsz': False,
[36m(main_task pid=3284823)[0m                                'ulysses_sequence_parallel_size': 1},
[36m(main_task pid=3284823)[0m                        'rollout': {'do_sample': True,
[36m(main_task pid=3284823)[0m                                    'dtype': 'bfloat16',
[36m(main_task pid=3284823)[0m                                    'enforce_eager': True,
[36m(main_task pid=3284823)[0m                                    'free_cache_engine': True,
[36m(main_task pid=3284823)[0m                                    'gpu_memory_utilization': 0.4,
[36m(main_task pid=3284823)[0m                                    'ignore_eos': False,
[36m(main_task pid=3284823)[0m                                    'load_format': 'dummy_dtensor',
[36m(main_task pid=3284823)[0m                                    'log_prob_max_token_len_per_gpu': 16384,
[36m(main_task pid=3284823)[0m                                    'log_prob_micro_batch_size': 4,
[36m(main_task pid=3284823)[0m                                    'log_prob_use_dynamic_bsz': False,
[36m(main_task pid=3284823)[0m                                    'max_num_batched_tokens': 8192,
[36m(main_task pid=3284823)[0m                                    'max_num_seqs': 1024,
[36m(main_task pid=3284823)[0m                                    'n': 5,
[36m(main_task pid=3284823)[0m                                    'name': 'vllm',
[36m(main_task pid=3284823)[0m                                    'prompt_length': 1523,
[36m(main_task pid=3284823)[0m                                    'response_length': 3046,
[36m(main_task pid=3284823)[0m                                    'temperature': 1.0,
[36m(main_task pid=3284823)[0m                                    'tensor_model_parallel_size': 2,
[36m(main_task pid=3284823)[0m                                    'top_k': -1,
[36m(main_task pid=3284823)[0m                                    'top_p': 1}},
[36m(main_task pid=3284823)[0m  'algorithm': {'adv_estimator': 'grpo',
[36m(main_task pid=3284823)[0m                'gamma': 1.0,
[36m(main_task pid=3284823)[0m                'kl_ctrl': {'kl_coef': 0.001, 'type': 'fixed'},
[36m(main_task pid=3284823)[0m                'kl_penalty': 'kl',
[36m(main_task pid=3284823)[0m                'lam': 1.0},
[36m(main_task pid=3284823)[0m  'critic': {'cliprange_value': 0.5,
[36m(main_task pid=3284823)[0m             'forward_max_token_len_per_gpu': 32768,
[36m(main_task pid=3284823)[0m             'forward_micro_batch_size': 64,
[36m(main_task pid=3284823)[0m             'grad_clip': 1.0,
[36m(main_task pid=3284823)[0m             'model': {'enable_gradient_checkpointing': False,
[36m(main_task pid=3284823)[0m                       'external_lib': None,
[36m(main_task pid=3284823)[0m                       'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=3284823)[0m                                       'grad_offload': False,
[36m(main_task pid=3284823)[0m                                       'optimizer_offload': False,
[36m(main_task pid=3284823)[0m                                       'param_offload': False,
[36m(main_task pid=3284823)[0m                                       'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=3284823)[0m                       'override_config': {},
[36m(main_task pid=3284823)[0m                       'path': '~/models/deepseek-llm-7b-chat',
[36m(main_task pid=3284823)[0m                       'tokenizer_path': 'Qwen/Qwen2.5-3B-Instruct',
[36m(main_task pid=3284823)[0m                       'use_remove_padding': False},
[36m(main_task pid=3284823)[0m             'optim': {'lr': 1e-05,
[36m(main_task pid=3284823)[0m                       'lr_warmup_steps_ratio': 0.0,
[36m(main_task pid=3284823)[0m                       'min_lr_ratio': None,
[36m(main_task pid=3284823)[0m                       'total_training_steps': -1,
[36m(main_task pid=3284823)[0m                       'warmup_style': 'constant'},
[36m(main_task pid=3284823)[0m             'ppo_epochs': 1,
[36m(main_task pid=3284823)[0m             'ppo_max_token_len_per_gpu': 32768,
[36m(main_task pid=3284823)[0m             'ppo_micro_batch_size': 64,
[36m(main_task pid=3284823)[0m             'ppo_mini_batch_size': 64,
[36m(main_task pid=3284823)[0m             'shuffle': False,
[36m(main_task pid=3284823)[0m             'strategy': 'fsdp',
[36m(main_task pid=3284823)[0m             'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=3284823)[0m             'use_dynamic_bsz': False},
[36m(main_task pid=3284823)[0m  'data': {'max_prompt_length': 1523,
[36m(main_task pid=3284823)[0m /data/yzeng58/anaconda3/envs/liftr/lib/python3.9/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(main_task pid=3284823)[0m No module named 'vllm._version'
[36m(main_task pid=3284823)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(pid=3291006)[0m /data/yzeng58/anaconda3/envs/liftr/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
[36m(pid=3291006)[0m   warnings.warn(
[36m(pid=3291006)[0m /data/yzeng58/anaconda3/envs/liftr/lib/python3.9/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=3291006)[0m No module named 'vllm._version'
[36m(pid=3291006)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(pid=3297266)[0m /data/yzeng58/anaconda3/envs/liftr/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
[36m(pid=3297266)[0m   warnings.warn(
[36m(pid=3297266)[0m /data/yzeng58/anaconda3/envs/liftr/lib/python3.9/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=3297266)[0m No module named 'vllm._version'
[36m(pid=3297266)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(WorkerDict pid=3297266)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(WorkerDict pid=3291006)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(WorkerDict pid=3291006)[0m Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00,  3.26it/s]
[36m(WorkerDict pid=3291006)[0m Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  3.28it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  3.27it/s]
[36m(WorkerDict pid=3297266)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=3291006)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(WorkerDict pid=3297266)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(WorkerDict pid=3297266)[0m Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00,  2.26it/s]
[36m(WorkerDict pid=3297266)[0m Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  2.45it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  2.42it/s]
[36m(WorkerDict pid=3297266)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(WorkerDict pid=3297266)[0m Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:04<00:04,  4.31s/it]
[36m(WorkerDict pid=3297266)[0m Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.24s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.40s/it]
[36m(WorkerDict pid=3291006)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=3291006)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(main_task pid=3284823)[0m           'max_response_length': 3046,
[36m(main_task pid=3284823)[0m           'prompt_key': 'prompt',
[36m(main_task pid=3284823)[0m           'return_raw_chat': False,
[36m(main_task pid=3284823)[0m           'return_raw_input_ids': False,
[36m(main_task pid=3284823)[0m           'tokenizer': None,
[36m(main_task pid=3284823)[0m           'train_batch_size': 128,
[36m(main_task pid=3284823)[0m           'train_files': '/data/yzeng58/liftr/datasets/blobs/50_shot/qwen-instruct/1000_samples_3.0_noise/train.parquet',
[36m(main_task pid=3284823)[0m           'val_batch_size': 640,
[36m(main_task pid=3284823)[0m           'val_files': '/data/yzeng58/liftr/datasets/blobs/50_shot/qwen-instruct/1000_samples_3.0_noise/test.parquet'},
[36m(main_task pid=3284823)[0m  'reward_model': {'enable': False,
[36m(main_task pid=3284823)[0m                   'forward_max_token_len_per_gpu': 32768,
[36m(main_task pid=3284823)[0m                   'max_length': None,
[36m(main_task pid=3284823)[0m                   'micro_batch_size': 64,
[36m(main_task pid=3284823)[0m                   'model': {'external_lib': None,
[36m(main_task pid=3284823)[0m                             'fsdp_config': {'min_num_params': 0,
[36m(main_task pid=3284823)[0m                                             'param_offload': False},
[36m(main_task pid=3284823)[0m                             'input_tokenizer': 'Qwen/Qwen2.5-3B-Instruct',
[36m(main_task pid=3284823)[0m                             'path': '~/models/FsfairX-LLaMA3-RM-v0.1',
[36m(main_task pid=3284823)[0m                             'use_remove_padding': False},
[36m(main_task pid=3284823)[0m                   'strategy': 'fsdp',
[36m(main_task pid=3284823)[0m                   'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=3284823)[0m                   'use_dynamic_bsz': False},
[36m(main_task pid=3284823)[0m  'trainer': {'critic_warmup': 0,
[36m(main_task pid=3284823)[0m              'default_hdfs_dir': None,
[36m(main_task pid=3284823)[0m              'default_local_dir': 'checkpoints/TinyZero/Qwen-Qwen2.5-3B-Instruct_blobs_50_shot_qwen-instruct_reslen_3046_nsamples_1000_noise_3.0',
[36m(main_task pid=3284823)[0m              'experiment_name': 'Qwen-Qwen2.5-3B-Instruct_blobs_50_shot_qwen-instruct_reslen_3046_nsamples_1000_noise_3.0',
[36m(main_task pid=3284823)[0m              'logger': ['wandb'],
[36m(main_task pid=3284823)[0m              'n_gpus_per_node': 2,
[36m(main_task pid=3284823)[0m              'nnodes': 1,
[36m(main_task pid=3284823)[0m              'project_name': 'TinyZero',
[36m(main_task pid=3284823)[0m              'save_freq': 10,
[36m(main_task pid=3284823)[0m              'test_freq': 10,
[36m(main_task pid=3284823)[0m              'total_epochs': 15,
[36m(main_task pid=3284823)[0m              'total_training_steps': None,
[36m(main_task pid=3284823)[0m              'val_before_train': False}}
[36m(main_task pid=3284823)[0m original dataset len: 800
[36m(main_task pid=3284823)[0m filter dataset len: 800
[36m(main_task pid=3284823)[0m original dataset len: 200
[36m(main_task pid=3284823)[0m filter dataset len: 200
[36m(main_task pid=3284823)[0m Size of train dataloader: 6
[36m(main_task pid=3284823)[0m Size of val dataloader: 1
[36m(main_task pid=3284823)[0m Total training steps: 90
[36m(WorkerDict pid=3291006)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=3291006)[0m   "_name_or_path": "Qwen/Qwen2.5-3B-Instruct",
[36m(WorkerDict pid=3291006)[0m   "architectures": [
[36m(WorkerDict pid=3291006)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=3291006)[0m   ],
[36m(WorkerDict pid=3291006)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=3291006)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=3291006)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=3291006)[0m   "hidden_size": 2048,
[36m(WorkerDict pid=3291006)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=3291006)[0m   "intermediate_size": 11008,
[36m(WorkerDict pid=3291006)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=3291006)[0m   "max_window_layers": 70,
[36m(WorkerDict pid=3291006)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=3291006)[0m   "num_attention_heads": 16,
[36m(WorkerDict pid=3291006)[0m   "num_hidden_layers": 36,
[36m(WorkerDict pid=3291006)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=3291006)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=3291006)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=3291006)[0m   "rope_scaling": null,
[36m(WorkerDict pid=3291006)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=3291006)[0m   "sliding_window": null,
[36m(WorkerDict pid=3291006)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=3291006)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=3291006)[0m   "transformers_version": "4.47.1",
[36m(WorkerDict pid=3291006)[0m   "use_cache": true,
[36m(WorkerDict pid=3291006)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=3291006)[0m   "vocab_size": 151936
[36m(WorkerDict pid=3291006)[0m }
[36m(WorkerDict pid=3291006)[0m 
[36m(WorkerDict pid=3291006)[0m NCCL version 2.20.5+cuda12.4
[36m(WorkerDict pid=3291006)[0m Qwen2ForCausalLM contains 3.09B parameters
[36m(WorkerDict pid=3291006)[0m wrap_policy: functools.partial(<function transformer_auto_wrap_policy at 0x7fa8c5450b80>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})
[36m(WorkerDict pid=3291006)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=3297266)[0m wrap_policy: functools.partial(<function transformer_auto_wrap_policy at 0x7f716d0ccb80>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})
[36m(WorkerDict pid=3291006)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=3291006)[0m   "_name_or_path": "Qwen/Qwen2.5-3B-Instruct",
[36m(WorkerDict pid=3291006)[0m   "architectures": [
[36m(WorkerDict pid=3291006)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=3291006)[0m   ],
[36m(WorkerDict pid=3291006)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=3291006)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=3291006)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=3291006)[0m   "hidden_size": 2048,
[36m(WorkerDict pid=3291006)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=3291006)[0m   "intermediate_size": 11008,
[36m(WorkerDict pid=3291006)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=3291006)[0m   "max_window_layers": 70,
[36m(WorkerDict pid=3291006)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=3291006)[0m   "num_attention_heads": 16,
[36m(WorkerDict pid=3291006)[0m   "num_hidden_layers": 36,
[36m(WorkerDict pid=3291006)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=3291006)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=3291006)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=3291006)[0m   "rope_scaling": null,
[36m(WorkerDict pid=3291006)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=3291006)[0m   "sliding_window": null,
[36m(WorkerDict pid=3291006)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=3291006)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=3291006)[0m   "transformers_version": "4.47.1",
[36m(WorkerDict pid=3291006)[0m   "use_cache": true,
[36m(WorkerDict pid=3291006)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=3291006)[0m   "vocab_size": 151936
[36m(WorkerDict pid=3291006)[0m }
[36m(WorkerDict pid=3291006)[0m 
[36m(WorkerDict pid=3291006)[0m Qwen2ForCausalLM contains 3.09B parameters
[36m(WorkerDict pid=3291006)[0m wrap_policy: functools.partial(<function transformer_auto_wrap_policy at 0x7fa8c5450b80>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})
[36m(WorkerDict pid=3297266)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=3291006)[0m Total steps: 90, num_warmup_steps: 0
[36m(WorkerDict pid=3291006)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=3297266)[0m /data/yzeng58/anaconda3/envs/liftr/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[36m(WorkerDict pid=3297266)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")
[36m(WorkerDict pid=3291006)[0m Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:04<00:04,  4.44s/it]
[36m(WorkerDict pid=3291006)[0m Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.23s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.41s/it]
[36m(WorkerDict pid=3297266)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")
[36m(WorkerDict pid=3291006)[0m /data/yzeng58/anaconda3/envs/liftr/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=3291006)[0m   warnings.warn(
[36m(WorkerDict pid=3291006)[0m /data/yzeng58/anaconda3/envs/liftr/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(WorkerDict pid=3291006)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")
[36m(WorkerDict pid=3291006)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")
[36m(main_task pid=3284823)[0m wandb: Currently logged in as: yuchen-zeng (lee-lab-uw-madison) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[36m(main_task pid=3284823)[0m wandb: Tracking run with wandb version 0.19.8
[36m(main_task pid=3284823)[0m wandb: Run data is saved locally in /data/yzeng58/liftr/wandb/run-20250323_210745-ck4b6b3p
[36m(main_task pid=3284823)[0m wandb: Run `wandb offline` to turn off syncing.
[36m(main_task pid=3284823)[0m wandb: Syncing run Qwen-Qwen2.5-3B-Instruct_blobs_50_shot_qwen-instruct_reslen_3046_nsamples_1000_noise_3.0
[36m(main_task pid=3284823)[0m wandb: ‚≠êÔ∏è View project at https://wandb.ai/lee-lab-uw-madison/TinyZero
[36m(main_task pid=3284823)[0m wandb: üöÄ View run at https://wandb.ai/lee-lab-uw-madison/TinyZero/runs/ck4b6b3p
[36m(WorkerDict pid=3291006)[0m /data/yzeng58/anaconda3/envs/liftr/lib/python3.9/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
[36m(WorkerDict pid=3291006)[0m   with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
[36m(WorkerDict pid=3297266)[0m /data/yzeng58/anaconda3/envs/liftr/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=3297266)[0m   warnings.warn(
[36m(WorkerDict pid=3291006)[0m Before building vllm rollout, memory allocated (GB): 5.779382228851318, memory reserved (GB): 10.130859375
[36m(WorkerDict pid=3291006)[0m INFO 03-23 21:07:23 config.py:887] Defaulting to use ray for distributed inference
[36m(WorkerDict pid=3291006)[0m WARNING 03-23 21:07:23 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(WorkerDict pid=3297266)[0m wrap_policy: functools.partial(<function transformer_auto_wrap_policy at 0x7f716d0ccb80>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})
[36m(WorkerDict pid=3297266)[0m Total steps: 90, num_warmup_steps: 0
[36m(WorkerDict pid=3297266)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=3297266)[0m local rank 0
[36m(WorkerDict pid=3297266)[0m INFO 03-23 21:07:23 selector.py:115] Using XFormers backend.
[36m(WorkerDict pid=3291006)[0m INFO 03-23 21:07:24 utils.py:1008] Found nccl from library libnccl.so.2
[36m(WorkerDict pid=3291006)[0m INFO 03-23 21:07:24 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(WorkerDict pid=3291006)[0m INFO 03-23 21:07:30 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7fa7d0899340>, local_subscribe_port=47959, remote_subscribe_port=None)
[36m(WorkerDict pid=3297266)[0m INFO 03-23 21:07:23 config.py:887] Defaulting to use ray for distributed inference
[36m(WorkerDict pid=3297266)[0m WARNING 03-23 21:07:23 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(WorkerDict pid=3291006)[0m local rank 0
[36m(WorkerDict pid=3291006)[0m INFO 03-23 21:07:23 selector.py:115] Using XFormers backend.
[36m(WorkerDict pid=3297266)[0m INFO 03-23 21:07:24 utils.py:1008] Found nccl from library libnccl.so.2
[36m(WorkerDict pid=3297266)[0m INFO 03-23 21:07:24 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(WorkerDict pid=3291006)[0m INFO 03-23 21:07:30 selector.py:115] Using XFormers backend.
[36m(WorkerDict pid=3291006)[0m before init cache memory allocated: 9.365187584GB, reserved: 9.430892544GB
[36m(WorkerDict pid=3297266)[0m INFO 03-23 21:07:30 selector.py:115] Using XFormers backend.
[36m(WorkerDict pid=3291006)[0m after init cache memory allocated: 20.689808384GB, reserved: 20.755513344GB
[36m(WorkerDict pid=3291006)[0m kwargs: {'n': 5, 'logprobs': 1, 'max_tokens': 3046, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}
[36m(WorkerDict pid=3291006)[0m After building vllm rollout, memory allocated (GB): 16.342007637023926, memory reserved (GB): 19.330078125
[36m(WorkerDict pid=3291006)[0m After building sharding manager, memory allocated (GB): 16.342007637023926, memory reserved (GB): 19.330078125
[36m(main_task pid=3284823)[0m epoch 0, step 1
[36m(main_task pid=3284823)[0m DataProto(batch=TensorDict(
[36m(main_task pid=3284823)[0m     fields={
[36m(main_task pid=3284823)[0m         attention_mask: Tensor(shape=torch.Size([128, 1523]), device=cpu, dtype=torch.int64, is_shared=False),
[36m(main_task pid=3284823)[0m         input_ids: Tensor(shape=torch.Size([128, 1523]), device=cpu, dtype=torch.int64, is_shared=False),
[36m(main_task pid=3284823)[0m         position_ids: Tensor(shape=torch.Size([128, 1523]), device=cpu, dtype=torch.int64, is_shared=False)},
[36m(main_task pid=3284823)[0m     batch_size=torch.Size([128]),
[36m(main_task pid=3284823)[0m     device=None,
[36m(main_task pid=3284823)[0m     is_shared=False), non_tensor_batch={}, meta_info={})
[36m(main_task pid=3284823)[0m DataProto(batch=TensorDict(
[36m(main_task pid=3284823)[0m     fields={
[36m(main_task pid=3284823)[0m         attention_mask: Tensor(shape=torch.Size([640, 4569]), device=cpu, dtype=torch.int64, is_shared=False),
[36m(main_task pid=3284823)[0m         input_ids: Tensor(shape=torch.Size([640, 4569]), device=cpu, dtype=torch.int64, is_shared=False),
[36m(main_task pid=3284823)[0m         old_log_probs: Tensor(shape=torch.Size([640, 3046]), device=cpu, dtype=torch.float32, is_shared=False),
[36m(main_task pid=3284823)[0m         position_ids: Tensor(shape=torch.Size([640, 4569]), device=cpu, dtype=torch.int64, is_shared=False),
[36m(main_task pid=3284823)[0m         prompts: Tensor(shape=torch.Size([640, 1523]), device=cpu, dtype=torch.int64, is_shared=False),
[36m(main_task pid=3284823)[0m         responses: Tensor(shape=torch.Size([640, 3046]), device=cpu, dtype=torch.int64, is_shared=False)},
[36m(main_task pid=3284823)[0m     batch_size=torch.Size([640]),
[36m(main_task pid=3284823)[0m     device=cpu,
[36m(main_task pid=3284823)[0m     is_shared=False), non_tensor_batch={}, meta_info={'micro_batch_size': 10, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0})
[36m(WorkerDict pid=3297266)[0m kwargs: {'n': 5, 'logprobs': 1, 'max_tokens': 3046, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}
Error executing job with overrides: ['algorithm.adv_estimator=grpo', 'data.train_files=/data/yzeng58/liftr/datasets/blobs/50_shot/qwen-instruct/1000_samples_3.0_noise/train.parquet', 'data.val_files=/data/yzeng58/liftr/datasets/blobs/50_shot/qwen-instruct/1000_samples_3.0_noise/test.parquet', 'data.train_batch_size=128', 'data.val_batch_size=640', 'data.max_prompt_length=1523', 'data.max_response_length=3046', 'actor_rollout_ref.model.path=Qwen/Qwen2.5-3B-Instruct', 'actor_rollout_ref.actor.optim.lr=1e-6', 'actor_rollout_ref.model.use_remove_padding=True', 'actor_rollout_ref.actor.ppo_mini_batch_size=64', 'actor_rollout_ref.actor.ppo_micro_batch_size=4', 'actor_rollout_ref.actor.use_kl_loss=True', 'actor_rollout_ref.actor.kl_loss_coef=0.001', 'actor_rollout_ref.actor.kl_loss_type=low_var_kl', 'actor_rollout_ref.model.enable_gradient_checkpointing=True', 'actor_rollout_ref.actor.fsdp_config.param_offload=False', 'actor_rollout_ref.actor.fsdp_config.grad_offload=False', 'actor_rollout_ref.actor.fsdp_config.optimizer_offload=False', 'actor_rollout_ref.rollout.log_prob_micro_batch_size=4', 'actor_rollout_ref.rollout.tensor_model_parallel_size=2', 'actor_rollout_ref.rollout.name=vllm', 'actor_rollout_ref.rollout.gpu_memory_utilization=0.4', 'actor_rollout_ref.rollout.n=5', 'actor_rollout_ref.ref.log_prob_micro_batch_size=2', 'actor_rollout_ref.ref.fsdp_config.param_offload=True', 'algorithm.kl_ctrl.kl_coef=0.001', 'trainer.critic_warmup=0', 'trainer.logger=[wandb]', '+trainer.val_before_train=False', 'trainer.default_hdfs_dir=null', 'trainer.n_gpus_per_node=2', 'trainer.nnodes=1', 'trainer.save_freq=10', 'trainer.test_freq=10', 'trainer.project_name=TinyZero', 'trainer.experiment_name=Qwen-Qwen2.5-3B-Instruct_blobs_50_shot_qwen-instruct_reslen_3046_nsamples_1000_noise_3.0', 'trainer.total_epochs=15']
Traceback (most recent call last):
  File "/data/yzeng58/liftr/verl/trainer/main_ppo.py", line 44, in main
    ray.get(main_task.remote(config))
  File "/data/yzeng58/anaconda3/envs/liftr/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/data/yzeng58/anaconda3/envs/liftr/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/data/yzeng58/anaconda3/envs/liftr/lib/python3.9/site-packages/ray/_private/worker.py", line 2771, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/data/yzeng58/anaconda3/envs/liftr/lib/python3.9/site-packages/ray/_private/worker.py", line 919, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::main_task()[39m (pid=3284823, ip=128.105.19.56)
  File "/data/yzeng58/liftr/verl/trainer/main_ppo.py", line 133, in main_task
    trainer.fit()
  File "/data/yzeng58/liftr/verl/trainer/ppo/ray_trainer.py", line 659, in fit
    actor_output = self.actor_rollout_wg.update_actor(batch)
  File "/data/yzeng58/liftr/verl/single_controller/ray/base.py", line 42, in func
    output = ray.get(output)
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::WorkerDict.actor_rollout_update_actor()[39m (pid=3291006, ip=128.105.19.56, actor_id=5520ac2ea96ea6cd123bde1c01000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7fa85ea11d90>)
  File "/data/yzeng58/liftr/verl/single_controller/ray/base.py", line 399, in func
    return getattr(self.worker_dict[key], name)(*args, **kwargs)
  File "/data/yzeng58/liftr/verl/single_controller/base/decorator.py", line 404, in inner
    return func(*args, **kwargs)
  File "/data/yzeng58/liftr/verl/workers/fsdp_workers.py", line 375, in update_actor
    metrics = self.actor.update_policy(data=data)
  File "/data/yzeng58/liftr/verl/workers/actor/dp_actor.py", line 272, in update_policy
    loss.backward()
  File "/data/yzeng58/anaconda3/envs/liftr/lib/python3.9/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/data/yzeng58/anaconda3/envs/liftr/lib/python3.9/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/data/yzeng58/anaconda3/envs/liftr/lib/python3.9/site-packages/torch/autograd/graph.py", line 768, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.68 GiB. GPU 0 has a total capacity of 39.50 GiB of which 4.14 GiB is free. Including non-PyTorch memory, this process has 33.50 GiB memory in use. Process 3448531 has 1.86 GiB memory in use. Of the allocated memory 29.38 GiB is allocated by PyTorch, and 1.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[1;34mwandb[0m: 
[1;34mwandb[0m: üöÄ View run [33msilvery-waterfall-27[0m at: [34mhttps://wandb.ai/lee-lab-uw-madison/liftr-rl/runs/trn9jewo[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250323_210616-trn9jewo/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: üöÄ View run [33mQwen-Qwen2.5-3B-Instruct_blobs_50_shot_qwen-instruct_reslen_3046_nsamples_1000_noise_3.0[0m at: [34mhttps://wandb.ai/lee-lab-uw-madison/TinyZero/runs/ck4b6b3p[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250323_210745-ck4b6b3p/logs[0m
[36m(WorkerDict pid=3297266)[0m /data/yzeng58/anaconda3/envs/liftr/lib/python3.9/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
[36m(WorkerDict pid=3297266)[0m   with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
